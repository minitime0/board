{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef437578-d709-4585-91b5-7fb487d42515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "http.client.HTTPResponse"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import sys\n",
    "\n",
    "sys.path # 모든 실행 경로 출력\n",
    "callable(len) # 실행 가능한경우 (함수, 클래스) True\n",
    "callable(len('asd')) # 값을 반환하는 경우에는 False\n",
    "type(print(\"abc\")) # print는 return이 없다. # NoneType\n",
    "type(urlopen(\"http://naver.com\")) # http.client.HTTPResponse\n",
    "dir(urlopen(\"http://naver.com\")) # 사용가능한 속성 목록\n",
    "html = urlopen(\"http://naver.com\") # <http.client.HTTPResponse at 0x1be6ff82aa0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7f9aa1-8e99-4025-900f-eb9a34d3174c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as req\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "\n",
    "## requests 이용\n",
    "html = req.get(\"http://www.pythonscraping.com/pages/page1.html\") # type(html) : requests.models.Response\n",
    "# dir(html) # 사용가능한 속성 목록출력\n",
    "# request는 get함수를 사용해서 파싱(html.parser)을 한번에 수행\n",
    "\n",
    "## urlopen 이용\n",
    "html2 = urlopen(\"http://www.pythonscraping.com/pages/page1.html\") # type(html2) : http.client.HTTPResponse\n",
    "# dir(html2) # 사용가능한 속성 목록출력\n",
    "\n",
    "bs = bfs(html2.read(), 'html.parser') # type(bs) : bs4.BeautifulSoup\n",
    "# BeautifulSoup(html, 'html.parser')도 동일하다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582faac2-a2fe-458c-b80c-e02244752267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\") # html.paser는 옵션\n",
    "res = html.read()\n",
    "bs = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "# find, findAll, find_all\n",
    "# bs.findAll('title')\n",
    "# bs.find('body').find('h1')\n",
    "# bs.find('div').text  \n",
    "# text는 태그로 감싸져 있는 내용(text)을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ebe228d-afe8-4227-985b-5d9298580274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Iron_Gwazi')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "# bs.html\n",
    "# {'class' : ['mw-file-description'] }\n",
    "# bs.find_all(attrs = {'class': ['mw-ui-icon-wikimedia-listBullet', 'vector-icon']}) # attrs에 딕셔너리 형태로 집어넣는다.\n",
    "# bs.find_all( {'class' : 'mw-ui-icon-wikimedia-listBullet'} ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c83ec83-436d-4d88-9537-95398e2e74cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL이 존재하지 않습니다. <urlopen error [Errno 11001] getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen(\"https://pythonscrapingthisurldoesnotexist.com\")\n",
    "except HTTPError as e:\n",
    "    print(\"통신 오류 입니다.\", e)\n",
    "except URLError as e:\n",
    "    print(\"URL이 존재하지 않습니다.\", e)\n",
    "else:\n",
    "    print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ba7df7d-10ea-45c5-998c-ab80da0d341f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NAVER'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.request import URLError, HTTPError\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "\n",
    "def get_title(url) :\n",
    "    try :\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e: # HTTPError 발생했을 경우\n",
    "        print(\"통신 오류 입니다.\", e)\n",
    "        return '' # '' 또는 None\n",
    "    except URLError as e: # URLError 발생했을 경우\n",
    "        print(\"URL이 존재하지 않습니다.\", e)\n",
    "        return '' # '' 또는 None\n",
    "    else:   \n",
    "        bs = bfs(html, 'html.parser')\n",
    "        try : \n",
    "            res = bs.find('title')\n",
    "            return res.text\n",
    "        except : # titlt이 존재하지 않는경우를 위한 예외처리\n",
    "            print('내용이 존재하지 않거나 잘못 입력하셨습니다.')\n",
    "            return ''  # '' 또는 None\n",
    "\n",
    "get_title(\"http://naver.com\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f20f2b3-2f3f-4706-972e-954c4e453162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs = bfs(html, \"html.parser\")\n",
    "# bs.html # 해당 사이트의 html 출력\n",
    "\n",
    "\n",
    "## span 태그 중에서 클래스 속성값이 green과 red인경우를 찾는다 \n",
    "name_list = bs.findAll('span', {'class': ['green', 'red']}) \n",
    "\n",
    "\n",
    "## name_list의 text 출력\n",
    "# for name in name_list:\n",
    "#     print(name.get_text())\n",
    "\n",
    "\n",
    "## find_all()함수에 들어가는 인자값 작성하는 형식\n",
    "# ('태그이름', {'속성' : ['속성값', '속성값'] } )\n",
    "# name_list_green = bs.find_all('span', {'class': 'green'})  \n",
    "# name_list_red = bs.find_all('span', {'class': 'red'}) \n",
    "# bs.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']) 내부에 리스트 형태로도 넣을 수 있다.\n",
    "\n",
    "\n",
    "## name_list_green리스트의 각 요소의 text값을 name_list_green_text 리스트에 집어넣기\n",
    "# name_list_green_text = []\n",
    "# for i in name_list_green :\n",
    "#     print(f'{i.text}\\n\\n')  # .text와 .get_text()는 같다.\n",
    "#     name_list_green_text.append(i)\n",
    "\n",
    "\n",
    "## 컴프리헨션(Comprehension)으로 표현하기\n",
    "# bs.find_all(['h' + str(i) for i in range(1,7)])\n",
    "# [text.get_text() for text in name_list]\n",
    "# [text.text for text in name_list]\n",
    "\n",
    "\n",
    "## text로 찾는방법\n",
    "# bs.find_all(string = 'the prince')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efb3b3da-460b-495d-ab9b-44049e62c28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$15.00'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = bfs(html, 'html.parser')\n",
    "\n",
    "## table 태그가 몇개 존재하는지 보기\n",
    "# len(bs.find_all('table'))\n",
    "\n",
    "\n",
    "## table태그 자식 출력\n",
    "# for child in bs.find('table',{'id':'giftList'}).children: \n",
    "#     print(child)\n",
    "\n",
    "\n",
    "## table태그 자식 인덱싱\n",
    "# bs.find_all('table',{'id':'giftList'})[0]\n",
    "\n",
    "\n",
    "## 제너레이터는 루프의 반복 동작을 제어할 수 있는 루틴 형태, 값이 여러개 존재한다는 것을 알 수 있다.\n",
    "## 이터레이터 예: range(1, 10), enumerate() (순서가 있는 경우라고 생각하면 된다.)\n",
    "\n",
    "\n",
    "## 자식 요소를 가져올때 (.children)\n",
    "# x = bs.find_all('table',{'id':'giftList'})[0].children\n",
    "\n",
    "\n",
    "## bs.find는 딕셔너리를 인덱싱하는 방식과 유사하다\n",
    "# bs.find('table').find('tr').find('th')\n",
    "# bs.find('table',{'id':'giftList'}).tr.th\n",
    "# bs.table.tr.th\n",
    "\n",
    "\n",
    "## tr요소의 그 다음형제(next.siblings)들 전부 출력 \n",
    "# list(bs.find('table',{'id':'giftList'}).tr.next_siblings)\n",
    "# len(bs.find('table',{'id':'giftList'}).tr.next_siblings) # 내 다음 형제 수\n",
    "\n",
    "\n",
    "## 달러 찾기\n",
    "# bs.find('table',{'id':'giftList'}).find_all('td')[2] \n",
    "\n",
    "\n",
    "## text값만 가져오기 (달러 숫자만 출력)\n",
    "# bs.findAll('tr')[1].find_all('td')[2].text.replace('\\n', '')\n",
    "\n",
    "\n",
    "## 내 부모의 앞에 있는 형제 (text(내용)의 부모가 td, td 앞의 형제의 text)\n",
    "# bs.find('img',{'src':'../img/gifts/img1.jpg'}).parent.previous_sibling.text.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f225dc4b-ae66-49ed-91a0-233f5ebe7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re # 정규표현식\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "## (\\ : 이스케이프, . : 한 글자를 의미 , * : 반복 여부 0번 또는 그이상\n",
    "images = bs.find_all('img', {'src':re.compile('\\.\\.\\/img\\/gifts/img.*\\.jpg')})\n",
    "# list(images)  ## 리스트 출력\n",
    "\n",
    "\n",
    "## src의 속성값만 출력 (딕셔너리를 인덱싱하는 방식과 유사) \n",
    "# for image in images: \n",
    "#     print(image['src'])\n",
    "\n",
    "\n",
    "## image태그에 속성과 속성값 추가 (딕셔너리의 .get()과 유사)\n",
    "# images[0].attrs('abc') = 'vv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faffce51-146e-488c-98fa-8c49be0773ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re # 정규표현식\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "images = bs.find_all('img', {'src':re.compile('\\.\\.\\/img\\/gifts/img.*\\.jpg')})\n",
    "\n",
    "## images리스트 요소의 속성부분을 딕셔너리로 출력 (attrs메소드, dir(image)로 메소드 확인가능)\n",
    "# for image in images :\n",
    "#     # print(type(image.attrs)) ## 딕셔너리 형태임을 알 수 있다.\n",
    "#     print(image.attrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7bc0a649-50c1-4825-b3b0-cd13605530bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re # 정규표현식\n",
    "\n",
    "# 이미지에 직접 접근하기위해 베이스 url설정\n",
    "base_url = 'http://www.pythonscraping.com'\n",
    "target_page = '/pages/page3.html'\n",
    "html = urlopen(base_url + target_page)\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "## 정규표현식 x*의미 = 반복여부를 표현하며 x 문자가 0번 또는 그 이상 반복됨을 의미한다.\n",
    "images = bs.find_all('img', {'src':re.compile('\\.\\.\\/img\\/gifts/img.*\\.jpg')})\n",
    "\n",
    "\n",
    "# .attrs은 딕셔너리 형태 이므로 .get()으로 접근 가능\n",
    "# type(images[0].attrs) ## 딕셔너리이다\n",
    "\n",
    "\n",
    "# images[0].attrs.get('src')[2:] ## ..부분 제외\n",
    "# base_url + images[0].attrs.get('src')[2:] # base_url과 합쳐서 이미지 절대경로에 접근한다.\n",
    "\n",
    "\n",
    "## 이미지 다운로드시 유용\n",
    "# for image in images :\n",
    "#     print(base_url + image.attrs.get('src')[2:]) \n",
    "\n",
    "\n",
    "## 응용 : 이미지 파일 다운로드하기\n",
    "# for idx, image in enumerate(images) :\n",
    "#     res = requests.get(base_url + image.attrs.get('src')[2:])\n",
    "#     with open(f'./img{idx+1}.jpg', 'wb') as file : ## img1.jpg 이름으로 저장\n",
    "#         file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0c992e7b-73d7-41c5-a5d2-a5c8743b6adc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img src=\"../img/gifts/logo.jpg\" style=\"float:left;\"/>,\n",
       " <tr class=\"gift\" id=\"gift1\"><td>\n",
       " Vegetable Basket\n",
       " </td><td>\n",
       " This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
       " <span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
       " </td><td>\n",
       " $15.00\n",
       " </td><td>\n",
       " <img src=\"../img/gifts/img1.jpg\"/>\n",
       " </td></tr>,\n",
       " <tr class=\"gift\" id=\"gift2\"><td>\n",
       " Russian Nesting Dolls\n",
       " </td><td>\n",
       " Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
       " </td><td>\n",
       " $10,000.52\n",
       " </td><td>\n",
       " <img src=\"../img/gifts/img2.jpg\"/>\n",
       " </td></tr>,\n",
       " <tr class=\"gift\" id=\"gift3\"><td>\n",
       " Fish Painting\n",
       " </td><td>\n",
       " If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
       " </td><td>\n",
       " $10,005.00\n",
       " </td><td>\n",
       " <img src=\"../img/gifts/img3.jpg\"/>\n",
       " </td></tr>,\n",
       " <tr class=\"gift\" id=\"gift4\"><td>\n",
       " Dead Parrot\n",
       " </td><td>\n",
       " This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
       " </td><td>\n",
       " $0.50\n",
       " </td><td>\n",
       " <img src=\"../img/gifts/img4.jpg\"/>\n",
       " </td></tr>,\n",
       " <tr class=\"gift\" id=\"gift5\"><td>\n",
       " Mystery Box\n",
       " </td><td>\n",
       " If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
       " </td><td>\n",
       " $1.50\n",
       " </td><td>\n",
       " <img src=\"../img/gifts/img6.jpg\"/>\n",
       " </td></tr>]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "## 속성의 갯수가 두개인 태그 전부 찾기\n",
    "# bs.find_all(lambda tag: len(tag.attrs) == 2)\n",
    "\n",
    "## 위에 람다 부분이 함수처럼 작동한다는 것을 알 수 있다. \n",
    "# def xx(x) :\n",
    "#     return len(x.attrs) == 2 ## len(x.attrs) == 2 일때만 리턴\n",
    "# bs.findAll(xx)\n",
    "\n",
    "## bs.findAll([True]) # True 일때만 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "18c79191-430c-45d0-85a2-f61a9bed2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "bs = bfs(html.read(), 'html.parser')\n",
    "\n",
    "## href속성을 가진 a태그만 links라는 리스트에 href의 속성값 저장\n",
    "# for link in bs.find_all('a'):\n",
    "#     if 'href' in link.attrs:\n",
    "#         links.append(link.attrs['href'])\n",
    "\n",
    "\n",
    "# links = []\n",
    "## 위와 결과값은 같은데 if문을 href속성값이 존재할때만(True) 리스트에 저장 하도록함\n",
    "# for link in bs.findAll('a') :\n",
    "#     if link.attrs.get('href') :\n",
    "#         links.append(link.attrs['href'])\n",
    "# print(len(links)) ## 962개\n",
    "\n",
    "\n",
    "# links2 = []\n",
    "## None값이 나오는 태그 확인하기\n",
    "# for link in bs.findAll('a') :\n",
    "#     if link.attrs.get('href') is not None :\n",
    "#         pass\n",
    "#     else :\n",
    "#         print(link.attrs.get('href')) \n",
    "#         links2.append(link.attrs.get('href'))\n",
    "# print(len(links2)) ## 5개\n",
    "\n",
    "\n",
    "# links3 = []\n",
    "## if문 없이 link.attrs.get('href')으로 접근하여 리스트에 값을 넣는경우\n",
    "## href속성이 없는 태그의 경우 link.attrs.get('href') None값을 반환하기 때문에 \n",
    "## 이 값이 리스트로 들어가면서 본래 리스트(links)보다 새 리스트(links1)길이가 긴것으로 보인다.\n",
    "# for link in bs.findAll('a') :\n",
    "#     links3.append(link.attrs.get('href'))\n",
    "# print(len(links3)) ## 967개\n",
    "\n",
    "\n",
    "## 위에 if문을 link.attrs['href'] 로 바꿀경우 try execpt 형식 사용하기\n",
    "## link.attrs[] 형태의 경우 href속성이 없는데 접근하려고 하면 키에러가 발생한다. 따라서 예외처리를 해주면 가능\n",
    "# for link in bs.findAll('a') :\n",
    "#     try :\n",
    "#         link.attrs['href']\n",
    "#         links.append(link.attrs['href'])\n",
    "#     except KeyError:\n",
    "#         pass   \n",
    "\n",
    "\n",
    "## 컴프리헨션(Comprehension)으로 표현하기\n",
    "# [link.attrs['href'] for link in bs.findAll('a') if link.attrs.get('href')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0b1f4644-9ae7-48ce-803b-931f78a30a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1571428571428575"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Alist :\n",
    "    # a = Alist자신([리스트]) a자체가 인자가 전부 들어간 함수가 되는것\n",
    "    def __init__(self, *args) : # 자기자신(self)을 인자로 받는다(자바에서는 this.), *args는 입력된 인자값들을 tuple로 묶어준다.\n",
    "        self.x = [] \n",
    "        for i in args :\n",
    "            if isinstance(i, (int, float)) : # isinstance(i, (int, float)) : i인자값이 (int, float)이면 True\n",
    "                self.x += [i] # self.x 리스트에 i를 넣는다.\n",
    "                \n",
    "    def sum(self) : # 덧셈\n",
    "        s = 0\n",
    "        for i in self.x :\n",
    "             s += i\n",
    "        self.s = s\n",
    "        return s\n",
    "        \n",
    "    def mean(self) : # 평균\n",
    "        return self.sum()/len(self.x)\n",
    "        \n",
    "    def append(self, val) : # 리스트 추가\n",
    "        self.x += [val]\n",
    "        \n",
    "    def extend(self, vals) : # 리스트 합치기\n",
    "        # hasattr([1](리스트), '__len__') # len이라는 속성(len이라는 함수를 쓸 수 있는지 확인)을 가지고 있는지 확인 T/F\n",
    "        if hasattr(vals, '__len__') :\n",
    "            for val in vals :\n",
    "                self.x += [val]\n",
    "        else :\n",
    "            self.x += [vals]\n",
    "            \n",
    "    def index1(self, val) : # index()함수 구현\n",
    "        self.idx = None\n",
    "        for idx, v in enumerate(self.x) :\n",
    "            if v == val :\n",
    "                self.idx = idx\n",
    "                break\n",
    "        return self.idx\n",
    "        \n",
    "    def pop(self, idx = -1) : # pop()함수 구현\n",
    "        a = self.x[idx]\n",
    "        print(a)\n",
    "        del self.x[idx]\n",
    "        return a\n",
    "        \n",
    "    def remove(self, val) : # remove는 중복된 값이 있을때 앞에 있는 값부터 지운다.\n",
    "        for idx, v in enumerate(self.x) :\n",
    "            if v == val :\n",
    "                del self.x[idx]\n",
    "                break\n",
    "                \n",
    "    def clear(self) : # 리스트값 전부 제거\n",
    "        for _ in range(len(self.x)) :\n",
    "            self.pop(0)\n",
    "            \n",
    "a = Alist(1,2,2,3,4,5,'a',5.1)\n",
    "# a.sum()\n",
    "\n",
    "# a.mean()\n",
    "# a.append(10)\n",
    "# a.mean()\n",
    "\n",
    "# a.extend([1,2,3])\n",
    "# a.x\n",
    "\n",
    "# a.x.index(2)\n",
    "# a.index1(2)\n",
    "## enumerate()의 두번째 인자값은 시작 번호를 지정한다 예를 들어 10이면 10부터 카운트한다.\n",
    "\n",
    "# a.x.pop(0)\n",
    "# a.pop(0)\n",
    "\n",
    "# a.remove(2)\n",
    "# a.remove(2)\n",
    "# a.x\n",
    "\n",
    "## dir(a.x) 사용 가능한 속성, 메소드 출력\n",
    "\n",
    "# a.clear()\n",
    "# a.x\n",
    "\n",
    "## shallow copy(얕은복사) : 복사한 본래 리스트를 가리키는 주소를 공유하지 않고 새로운 주소를 가지는 리스트를 생성한다.(주소가 다르기때문에 기존값에 영향을 주지 않고 독립적이다.)\n",
    "## deep copy(깊은복사) : 복사한 본래 리스트를 가리키는 주소를 공유한다\n",
    "## https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQfn4P%2FbtraPuqFuwf%2F1DvU9UA9eP5EZSsoeLz70k%2Fimg.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f8884-31c2-42ee-9bbf-b684404ffa1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
